{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"nLJkX47Bg-K4","cell_type":"markdown","source":["## Fine tune Phi-3 and Llama for sequence classification\n","\n","Fine tune to classify stress or no stress using messages from Dreaddit: A Reddit Dataset for Stress Analysis in Social Media.\n","See: https://aclanthology.org/D19-6213\n","\n","Commonly updated parameters are in the Parameter block below.\n","\n","**test_count:** The number of messages to use in generating prompts.\n","\n","**model_name** The model ID downloaded from Hugging Face.\n","\n","**access_token** The Hugging Face access token.\n","\n","Note, to work with batched input had to set padding=True when tokenizing input. Training loop handles the padded tokens correctly by masking them during the loss computation: attention_mask=ds_tokenized[\"train\"][\"attention_mask\"]\n","\n","Training data was reduced to 1200 to not exhaust GPU memory on Kaggle."],"metadata":{"id":"nLJkX47Bg-K4"}},{"id":"8ec99e5d-039e-421c-bc6e-eed3a2e78a83","cell_type":"markdown","source":["#### Parameters"],"metadata":{"id":"8ec99e5d-039e-421c-bc6e-eed3a2e78a83"}},{"id":"31c2bf66-fe82-4bc4-a031-f438079af42d","cell_type":"code","source":["test_count = 1200\n","model_name  = \"microsoft/Phi-3-medium-4K-instruct\"\n","#model_name =  \"meta-llama/Llama-2-7b-hf\"\n","#model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n","access_token = ''"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:48:09.750498Z","iopub.execute_input":"2025-04-20T11:48:09.750725Z","iopub.status.idle":"2025-04-20T11:48:09.754790Z","shell.execute_reply.started":"2025-04-20T11:48:09.750697Z","shell.execute_reply":"2025-04-20T11:48:09.753651Z"},"id":"31c2bf66-fe82-4bc4-a031-f438079af42d"},"outputs":[],"execution_count":null},{"id":"be5d91ed-7501-4d8c-8d4a-56cd4cf6e24a","cell_type":"markdown","source":[],"metadata":{"id":"be5d91ed-7501-4d8c-8d4a-56cd4cf6e24a"}},{"id":"K0ESnwEl0fx1","cell_type":"code","source":["!pip install transformers\n","!pip install torch # torch\n","!pip install peft # necessary for finetuning of the large model via LoRA approach\n","!pip install bitsandbytes # necessary for quantiziation\n","!pip install evaluate # extension of the transformers library\n","!pip install datasets # extension of the transformers library\n","!pip install accelerate"],"metadata":{"id":"K0ESnwEl0fx1","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:48:09.755730Z","iopub.execute_input":"2025-04-20T11:48:09.756066Z","iopub.status.idle":"2025-04-20T11:48:37.315725Z","shell.execute_reply.started":"2025-04-20T11:48:09.756032Z","shell.execute_reply":"2025-04-20T11:48:37.314571Z"}},"outputs":[],"execution_count":null},{"id":"92h2kCYH0q33","cell_type":"code","source":["import torch\n","import pandas as pd\n","from datasets import Dataset, load_dataset, DatasetDict\n","from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n","from transformers import (\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    EarlyStoppingCallback,\n","    DataCollatorWithPadding,\n","    TextClassificationPipeline)\n","from sklearn.model_selection import train_test_split\n","\n","import bitsandbytes as bnb\n","\n","import evaluate\n","import numpy as np\n","import time\n","import random"],"metadata":{"id":"92h2kCYH0q33","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:48:37.317110Z","iopub.execute_input":"2025-04-20T11:48:37.317500Z","iopub.status.idle":"2025-04-20T11:49:00.558535Z","shell.execute_reply.started":"2025-04-20T11:48:37.317470Z","shell.execute_reply":"2025-04-20T11:49:00.557663Z"}},"outputs":[],"execution_count":null},{"id":"lgeTaKIxRcMr","cell_type":"code","source":["# Set start time for training\n","start_time = time.time()"],"metadata":{"id":"lgeTaKIxRcMr","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:00.559560Z","iopub.execute_input":"2025-04-20T11:49:00.560292Z","iopub.status.idle":"2025-04-20T11:49:00.563859Z","shell.execute_reply.started":"2025-04-20T11:49:00.560254Z","shell.execute_reply":"2025-04-20T11:49:00.562925Z"}},"outputs":[],"execution_count":null},{"id":"chHcNT6K06kc","cell_type":"markdown","source":["### Logon to HuggingFace"],"metadata":{"id":"chHcNT6K06kc"}},{"id":"ahEgRnlB0-RO","cell_type":"code","source":["### the safer way\n","#from huggingface_hub import notebook_login\n","#notebook_login()\n","\n","### alternative\n","from huggingface_hub import login\n","login(access_token)"],"metadata":{"id":"ahEgRnlB0-RO","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:00.564837Z","iopub.execute_input":"2025-04-20T11:49:00.565186Z","iopub.status.idle":"2025-04-20T11:49:00.722183Z","shell.execute_reply.started":"2025-04-20T11:49:00.565106Z","shell.execute_reply":"2025-04-20T11:49:00.721436Z"}},"outputs":[],"execution_count":null},{"id":"7ec5EzcFC__S","cell_type":"markdown","source":["### Load messages, dropping nulls.\n"],"metadata":{"id":"7ec5EzcFC__S"}},{"id":"0VIXwvlosv-7","cell_type":"code","source":["# URL of the raw CSV file on GitHub\n","csv_url = \"https://raw.githubusercontent.com/SocialHealthAI/SDOH-Models/refs/heads/main/LLM%20Classification/dreaddit-train.csv\"\n","\n","# Load the CSV file into a DataFrame\n","train_df = pd.read_csv(csv_url)\n","train_df = train_df.head(test_count)\n","#dreadit_text_df = dreadit_df[['text']]\n","\n","# URL of the raw CSV file on GitHub\n","csv_url = \"https://raw.githubusercontent.com/SocialHealthAI/SDOH-Models/refs/heads/main/LLM%20Classification/dreaddit-test.csv\"\n","\n","# Load the CSV file into a DataFrame\n","test_df = pd.read_csv(csv_url)\n","test_df = test_df.head(400)\n","#dreadit_text_df = dreadit_df[['text']]\n","\n","# drop null text\n","train_df.dropna(subset=['text'], inplace=True)\n","test_df.dropna(subset=['text'], inplace=True)\n"],"metadata":{"id":"0VIXwvlosv-7","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:00.724867Z","iopub.execute_input":"2025-04-20T11:49:00.725084Z","iopub.status.idle":"2025-04-20T11:49:01.526560Z","shell.execute_reply.started":"2025-04-20T11:49:00.725066Z","shell.execute_reply":"2025-04-20T11:49:01.525589Z"}},"outputs":[],"execution_count":null},{"id":"tFfIldfMUfvZ","cell_type":"code","source":["label_names = ['0', '1']"],"metadata":{"id":"tFfIldfMUfvZ","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:01.528127Z","iopub.execute_input":"2025-04-20T11:49:01.528415Z","iopub.status.idle":"2025-04-20T11:49:01.532317Z","shell.execute_reply.started":"2025-04-20T11:49:01.528392Z","shell.execute_reply":"2025-04-20T11:49:01.531380Z"}},"outputs":[],"execution_count":null},{"id":"KcFt8-MIcXs2","cell_type":"markdown","source":["### Split test data into test and validation data"],"metadata":{"id":"KcFt8-MIcXs2"}},{"id":"NhXjYIxkcbS-","cell_type":"code","source":["# Calculate the split index\n","split_index = len(test_df) // 2\n","\n","# Split the DataFrame\n","validation_df = test_df.iloc[:split_index]\n","test_df = test_df.iloc[split_index:]\n"],"metadata":{"id":"NhXjYIxkcbS-","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:01.533214Z","iopub.execute_input":"2025-04-20T11:49:01.533624Z","iopub.status.idle":"2025-04-20T11:49:01.546000Z","shell.execute_reply.started":"2025-04-20T11:49:01.533542Z","shell.execute_reply":"2025-04-20T11:49:01.545319Z"}},"outputs":[],"execution_count":null},{"id":"ZDGYKunVhlQx","cell_type":"markdown","source":["### Create DatasetDict"],"metadata":{"id":"ZDGYKunVhlQx"}},{"id":"RwHeqnwLaUJd","cell_type":"code","source":["train_body = train_df['text'].values.tolist()\n","train_label = np.array(train_df['label'].values.tolist())\n","\n","val_body = validation_df['text'].values.tolist()\n","val_label = np.array(validation_df['label'].values.tolist())\n","\n","test_body = test_df['text'].values.tolist()\n","test_label = np.array(test_df['label'].values.tolist())\n","\n","# create hf dataset\n","ds = DatasetDict({\n","    'train': Dataset.from_dict({'body': train_body, 'label': train_label}),\n","    'val': Dataset.from_dict({'body': val_body, 'label': val_label}),\n","    'test': Dataset.from_dict({'body': test_body, 'label': test_label})\n","})"],"metadata":{"id":"RwHeqnwLaUJd","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:01.546699Z","iopub.execute_input":"2025-04-20T11:49:01.546885Z","iopub.status.idle":"2025-04-20T11:49:01.580941Z","shell.execute_reply.started":"2025-04-20T11:49:01.546868Z","shell.execute_reply":"2025-04-20T11:49:01.580306Z"}},"outputs":[],"execution_count":null},{"id":"JHjKw0R0JwyJ","cell_type":"code","source":["ds['train'][0]"],"metadata":{"id":"JHjKw0R0JwyJ","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:01.581653Z","iopub.execute_input":"2025-04-20T11:49:01.581857Z","iopub.status.idle":"2025-04-20T11:49:01.590220Z","shell.execute_reply.started":"2025-04-20T11:49:01.581840Z","shell.execute_reply":"2025-04-20T11:49:01.589641Z"}},"outputs":[],"execution_count":null},{"id":"XrryyoPr3daK","cell_type":"markdown","source":["### Tokenization and Quantization\n","Load tokenizer.\n","\n","Define tokenizer function with truncation=True so sequences are truncated to a specific length (often used for training).  \n","\n","Use the map function from the Datasets library to apply the preprocess_function to IMDb dataset in batches for efficiency. This creates a new dataset named tokenized_imdb with additional columns:\n","\n","* input_ids: Numerical representation of the text using tokenizer vocabulary.\n","* attention_mask: Mask to indicate valid elements in padded sequences."],"metadata":{"id":"XrryyoPr3daK"}},{"id":"aMYV7Y4z3k00","cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token                                         # added\n","print(f' Vocab size of the model {model_name}: {len(tokenizer.get_vocab())}')\n","#Vocab size of the model google/gemma-2b-it: 256000\n","\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"body\"], truncation=True, padding=True)\n","\n","ds_tokenized = ds.map(preprocess_function, batched=True)"],"metadata":{"id":"aMYV7Y4z3k00","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:01.590987Z","iopub.execute_input":"2025-04-20T11:49:01.591210Z","iopub.status.idle":"2025-04-20T11:49:04.451209Z","shell.execute_reply.started":"2025-04-20T11:49:01.591191Z","shell.execute_reply":"2025-04-20T11:49:04.450348Z"}},"outputs":[],"execution_count":null},{"id":"2X7ap4pi6QbZ","cell_type":"markdown","source":["### Label mapping\n","create dictionaries to map labels (text) to numerical IDs and vice versa:"],"metadata":{"id":"2X7ap4pi6QbZ"}},{"id":"G0VgRqv-6axn","cell_type":"code","source":["id2label = {0: \"Not Stressed\", 1: \"Stressed\"}\n","label2id = {\"Not Stressed\": 0, \"Stressed\": 1}"],"metadata":{"id":"G0VgRqv-6axn","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:04.452279Z","iopub.execute_input":"2025-04-20T11:49:04.452636Z","iopub.status.idle":"2025-04-20T11:49:04.456792Z","shell.execute_reply.started":"2025-04-20T11:49:04.452592Z","shell.execute_reply":"2025-04-20T11:49:04.455798Z"}},"outputs":[],"execution_count":null},{"id":"PwpPphTk6l8J","cell_type":"markdown","source":["### Data Collator\n","The DataCollatorWithPadding class from Transformers helps prepare batches of data for training. It handles padding sequences to a common length and creates attention masks. We can simply instantiate it with the tokenizer"],"metadata":{"id":"PwpPphTk6l8J"}},{"id":"uyHPHQej6vlR","cell_type":"code","source":["data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"uyHPHQej6vlR","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:04.457547Z","iopub.execute_input":"2025-04-20T11:49:04.457747Z","iopub.status.idle":"2025-04-20T11:49:04.475984Z","shell.execute_reply.started":"2025-04-20T11:49:04.457729Z","shell.execute_reply":"2025-04-20T11:49:04.475064Z"}},"outputs":[],"execution_count":null},{"id":"89vpgDA9684o","cell_type":"markdown","source":["### Evaluation Metrics\n","To assess the performance of our fine-tuned GEMMA-2b model use the evaluate library. This library provides convenient functions for calculating various evaluation metrics commonly used in classification tasks.\n","\n","Define a function compute_metrics that takes the model's predictions and ground-truth labels as input and calculates several metrics"],"metadata":{"id":"89vpgDA9684o"}},{"id":"AXy0CTzx7RNY","cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)  # Convert logits to class predictions\n","    accuracy = accuracy_score(labels, predictions)\n","    f1 = f1_score(labels, predictions, average=\"weighted\")\n","    return {\"accuracy\": accuracy, \"f1\": f1}\n"],"metadata":{"id":"AXy0CTzx7RNY","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:04.476788Z","iopub.execute_input":"2025-04-20T11:49:04.477020Z","iopub.status.idle":"2025-04-20T11:49:04.485913Z","shell.execute_reply.started":"2025-04-20T11:49:04.476990Z","shell.execute_reply":"2025-04-20T11:49:04.484999Z"}},"outputs":[],"execution_count":null},{"id":"JE8w1YGJ7gdH","cell_type":"markdown","source":["#Quantization Configuration\n","Transformers library provides the BitsAndBytesConfig class for defining quantization parameters."],"metadata":{"id":"JE8w1YGJ7gdH"}},{"id":"wRgPQaBB7rkf","cell_type":"code","source":["bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,  # Enables 4-bit quantization\n","    bnb_4bit_use_double_quant=True,  # Use double quantization for potentially higher accuracy (optional)\n","    bnb_4bit_quant_type=\"nf4\",  # Quantization type (specifics depend on hardware and library)\n","    bnb_4bit_compute_dtype=torch.bfloat16  # Compute dtype for improved efficiency (optional)\n",")"],"metadata":{"id":"wRgPQaBB7rkf","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:04.486920Z","iopub.execute_input":"2025-04-20T11:49:04.487245Z","iopub.status.idle":"2025-04-20T11:49:04.501415Z","shell.execute_reply.started":"2025-04-20T11:49:04.487198Z","shell.execute_reply":"2025-04-20T11:49:04.500741Z"}},"outputs":[],"execution_count":null},{"id":"_FmS9ss377_4","cell_type":"markdown","source":["### Load model in 4-bit\n"],"metadata":{"id":"_FmS9ss377_4"}},{"id":"8Jox6MOY7_pQ","cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    problem_type=\"single_label_classification\",                                # added, why? its binary\n","    num_labels=2,  # Number of output labels\n","    id2label=id2label,\n","    label2id=label2id,\n","    quantization_config=bnb_config,  # configuration for quantization\n","    device_map={\"\": 0}  # Optional dictionary specifying device mapping (single GPU with index 0 here)\n",")"],"metadata":{"id":"8Jox6MOY7_pQ","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:49:04.502307Z","iopub.execute_input":"2025-04-20T11:49:04.502542Z","iopub.status.idle":"2025-04-20T11:53:23.798969Z","shell.execute_reply.started":"2025-04-20T11:49:04.502521Z","shell.execute_reply":"2025-04-20T11:53:23.798306Z"}},"outputs":[],"execution_count":null},{"id":"GkR5c4cq_Bma","cell_type":"markdown","source":["### Gradient checkpointing\n","Memory optimization technique that can be helpful for large models."],"metadata":{"id":"GkR5c4cq_Bma"}},{"id":"NGsyPEX0--j-","cell_type":"code","source":["model.gradient_checkpointing_enable()"],"metadata":{"id":"NGsyPEX0--j-","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:53:23.799761Z","iopub.execute_input":"2025-04-20T11:53:23.799981Z","iopub.status.idle":"2025-04-20T11:53:23.805018Z","shell.execute_reply.started":"2025-04-20T11:53:23.799953Z","shell.execute_reply":"2025-04-20T11:53:23.804302Z"}},"outputs":[],"execution_count":null},{"id":"7JJyTxvY_YBl","cell_type":"markdown","source":["### prepare model for quantization"],"metadata":{"id":"7JJyTxvY_YBl"}},{"id":"HIgsLeW9_b3d","cell_type":"code","source":["model = prepare_model_for_kbit_training(model)"],"metadata":{"id":"HIgsLeW9_b3d","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:53:23.805945Z","iopub.execute_input":"2025-04-20T11:53:23.806252Z","iopub.status.idle":"2025-04-20T11:53:23.849877Z","shell.execute_reply.started":"2025-04-20T11:53:23.806202Z","shell.execute_reply":"2025-04-20T11:53:23.849295Z"}},"outputs":[],"execution_count":null},{"id":"9YBo6WnNAGXb","cell_type":"markdown","source":["### Layer names are needed for the LoRA configuration."],"metadata":{"id":"9YBo6WnNAGXb"}},{"id":"CoC5vYbK_vFd","cell_type":"code","source":["def find_linear_names(model):\n","    \"\"\"\n","    This function identifies all linear layer names within a model that use 4-bit quantization.\n","    Args:\n","        model (torch.nn.Module): The PyTorch model to inspect.\n","    Returns:\n","        list: A list containing the names of all identified linear layers with 4-bit quantization.\n","    \"\"\"\n","    cls = bnb.nn.Linear4bit\n","\n","    # Set to store identified layer names\n","    lora_module_names = set()\n","\n","    # Iterate through named modules in the model\n","    for name, module in model.named_modules():\n","        # Check if the current module is an instance of the 4-bit linear layer class\n","        if isinstance(module, cls):\n","            names = name.split('.')\n","            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n","\n","        # Special case: remove 'lm_head' if present\n","        if 'lm_head' in lora_module_names:\n","            lora_module_names.remove('lm_head')\n","    return list(lora_module_names)\n","\n","# Example usage:\n","modules = find_linear_names(model)\n","print(modules)\n","# ['down_proj', 'gate_proj', 'q_proj', 'o_proj', 'up_proj', 'v_proj', 'k_proj']"],"metadata":{"id":"CoC5vYbK_vFd","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:53:23.850581Z","iopub.execute_input":"2025-04-20T11:53:23.850788Z","iopub.status.idle":"2025-04-20T11:53:23.857076Z","shell.execute_reply.started":"2025-04-20T11:53:23.850770Z","shell.execute_reply":"2025-04-20T11:53:23.856322Z"}},"outputs":[],"execution_count":null},{"id":"Im9YlEex-3CO","cell_type":"markdown","source":["### LoRA Configuration\n","LoRA is a technique that minimizes the number of parameters requiring training during fine-tuning by keeping all original model parameters frozen and introducing a pair of rank decomposition matrices alongside the existing weights. These smaller matrices are designed so that their product matches the dimensions of the weights they modify. The original weights of the LLM remain unchanged, while the smaller matrices are trained using supervised learning. During inference, the two low-rank matrices are multiplied to form a matrix that matches the dimensions of the frozen weights. This matrix is then added to the original weights, effectively updating them in the model.\n","\n","Use a rank of 8 to train two small rank decomposition matrices with dimensions 8 by A and 8 by B whose product provides a matrix with same dimensions as the frozen weights.  Use task type **SEQ\\_CLS (Sequence Classification)** for text classification tasks\n"],"metadata":{"id":"Im9YlEex-3CO"}},{"id":"IGhuYXndyJWA","cell_type":"code","source":["lora_config = LoraConfig(\n","    r=16,  # 32 to 16 to reduce GPU memory usage\n","    lora_alpha=32,  # Dimensionality of the adapter projection\n","    target_modules=modules,  # List of modules to apply the LoRA adapter\n","    lora_dropout=0.05,  # Dropout rate for the adapter\n","    bias=\"none\",  # Bias configuration for the adapter\n","    task_type=\"SEQ_CLS\"  # Task type (sequence classification in this case)\n",")\n","\n","model = get_peft_model(model, lora_config)\n","model.print_trainable_parameters()"],"metadata":{"id":"IGhuYXndyJWA","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:53:23.860439Z","iopub.execute_input":"2025-04-20T11:53:23.860690Z","iopub.status.idle":"2025-04-20T11:53:24.623195Z","shell.execute_reply.started":"2025-04-20T11:53:23.860658Z","shell.execute_reply":"2025-04-20T11:53:24.622318Z"}},"outputs":[],"execution_count":null},{"id":"ans0UY5ZvhKi","cell_type":"markdown","source":["### Define trainer arguments"],"metadata":{"id":"ans0UY5ZvhKi"}},{"id":"ByCLTgaevkTp","cell_type":"code","source":["training_args = TrainingArguments(\n","    logging_steps=10,                                                                 # added\n","    output_dir=\"epoch_weights\",  # Output directory for checkpoints\n","    learning_rate=5e-5,  # Learning rate for the optimizer,                           # 2e-5 to 5e-5\n","    per_device_train_batch_size=10,  # Batch size per device                           # 20 to 10 to reduce GPU memory useage\n","    per_device_eval_batch_size=10,  # Batch size per device for evaluation             # ditto\n","    num_train_epochs=4,  # Number of training epochs from 5 to 1 (list error after 2)\n","    weight_decay=0.01,  # Weight decay for regularization\n","    evaluation_strategy='epoch',  # Evaluate after each epoch\n","    save_strategy=\"epoch\",  # Save model checkpoints after each epoch\n","    load_best_model_at_end=True,  # Load the best model based on the chosen metric\n","    push_to_hub=False,  # Disable pushing the model to the Hugging Face Hub\n","    report_to=\"none\",  # Disable logging to Weight&Bias\n","    metric_for_best_model='eval_loss')  # Metric for selecting the best model"],"metadata":{"id":"ByCLTgaevkTp","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:53:24.624543Z","iopub.execute_input":"2025-04-20T11:53:24.624772Z","iopub.status.idle":"2025-04-20T11:53:24.661520Z","shell.execute_reply.started":"2025-04-20T11:53:24.624751Z","shell.execute_reply":"2025-04-20T11:53:24.660750Z"}},"outputs":[],"execution_count":null},{"id":"QTXJYf6ewKoi","cell_type":"markdown","source":["### Early stopping\n","Automate epoch determination to prevent overfitting by stopping training if the validation performance doesnâ€™t improve for a certain number of epochs."],"metadata":{"id":"QTXJYf6ewKoi"}},{"id":"hLgKlYc6wMJw","cell_type":"code","source":["early_stop = EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=.0)"],"metadata":{"id":"hLgKlYc6wMJw","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:53:24.662221Z","iopub.execute_input":"2025-04-20T11:53:24.662554Z","iopub.status.idle":"2025-04-20T11:53:24.666340Z","shell.execute_reply.started":"2025-04-20T11:53:24.662528Z","shell.execute_reply":"2025-04-20T11:53:24.665481Z"}},"outputs":[],"execution_count":null},{"id":"px_IYY8LpCC7","cell_type":"markdown","source":[],"metadata":{"id":"px_IYY8LpCC7"}},{"id":"gCrHN-eTxY6T","cell_type":"markdown","source":["### Define trainer and train"],"metadata":{"id":"gCrHN-eTxY6T"}},{"id":"iwsee4u8xcnV","cell_type":"code","source":["# Make sure the model recognizes the pad_token_id\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","trainer = Trainer(\n","    model=model,  # The LoRA-adapted model\n","    args=training_args,  # Training arguments\n","    train_dataset=ds_tokenized[\"train\"],  # Training dataset\n","    eval_dataset=ds_tokenized[\"val\"],  # Evaluation dataset\n","    tokenizer=tokenizer,  # Tokenizer for processing text\n","    data_collator=data_collator,  # Data collator for preparing batches\n","    compute_metrics=compute_metrics,  # Function to calculate evaluation metrics\n","    callbacks=[early_stop],  # Optional early stopping callback\n","    #attention_mask=ds_tokenized[\"train\"][\"attention_mask\"]\n",")\n","\n","trainer.train()"],"metadata":{"id":"iwsee4u8xcnV","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:53:24.667063Z","iopub.execute_input":"2025-04-20T11:53:24.667341Z"}},"outputs":[],"execution_count":null},{"id":"H2k0jMFtQ5c_","cell_type":"code","source":["# Calculate and print the elapsed time\n","elapsed_time = time.time() - start_time\n","print(f\"Time required by training: {elapsed_time:.4f} seconds\")"],"metadata":{"id":"H2k0jMFtQ5c_","trusted":true},"outputs":[],"execution_count":null},{"id":"50ea0279-1c5d-48c9-a448-ba2b47c09d2d","cell_type":"markdown","source":["### Save model and tokenizer"],"metadata":{"id":"50ea0279-1c5d-48c9-a448-ba2b47c09d2d"}},{"id":"e7c04828-6aa8-4fb7-b5b0-e1facf23a7ef","cell_type":"code","source":["# Define the directory to save the model and tokenizer\n","save_directory = \"./fine_tuned_bert_model\"\n","\n","# Save the model and tokenizer\n","model.save_pretrained(save_directory)\n","tokenizer.save_pretrained(save_directory)\n","\n","print(f\"Model and tokenizer saved to {save_directory}\")"],"metadata":{"trusted":true,"id":"e7c04828-6aa8-4fb7-b5b0-e1facf23a7ef"},"outputs":[],"execution_count":null},{"id":"cc806bb9-1fa8-473e-a935-524330d18ed5","cell_type":"markdown","source":["### Starting time for Inference"],"metadata":{"id":"cc806bb9-1fa8-473e-a935-524330d18ed5"}},{"id":"a0d652e1-ab6a-436a-8c97-58ccf7af4b7d","cell_type":"code","source":["# Set start time for Inference\n","start_time = time.time()"],"metadata":{"trusted":true,"id":"a0d652e1-ab6a-436a-8c97-58ccf7af4b7d"},"outputs":[],"execution_count":null},{"id":"05f88096-2138-4b71-afff-1db4a8022814","cell_type":"markdown","source":["### Load model and tokenizer"],"metadata":{"id":"05f88096-2138-4b71-afff-1db4a8022814"}},{"id":"514b726d-5559-486a-8fc7-3e653578a8ea","cell_type":"code","source":["# Reload the model and tokenizer\n","#reloaded_model = BertForSequenceClassification.from_pretrained(save_directory)\n","reloaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n","#reloaded_tokenizer = BertTokenizer.from_pretrained(save_directory)\n","reloaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)"],"metadata":{"trusted":true,"id":"514b726d-5559-486a-8fc7-3e653578a8ea"},"outputs":[],"execution_count":null},{"id":"2ec4e500-7f90-4b98-95cf-94f995728b32","cell_type":"markdown","source":["### Inference on Test data"],"metadata":{"id":"2ec4e500-7f90-4b98-95cf-94f995728b32"}},{"id":"9f93f3ee-23b1-4f27-98f7-2fb9d6ecedad","cell_type":"code","source":["def predict(input_text):\n","    \"\"\"\n","    Predicts the sentiment label for a given text input.\n","    Args:\n","        input_text (str): The text to predict the sentiment for.\n","    Returns:\n","        float: The predicted probability of the text being positive sentiment.\n","    \"\"\"\n","    inputs = reloaded_tokenizer(input_text, return_tensors=\"pt\")  # Convert to PyTorch tensors and move to GPU (if available)\n","    with torch.no_grad():\n","        outputs = reloaded_model(**inputs).logits  # Get the model's output logits\n","    y_prob = torch.sigmoid(outputs).tolist()[0]  # Apply sigmoid activation and convert to list\n","    #return np.round(y_prob, 5)  # Round the predicted probability to 5 decimal places\n","    return y_prob[1]  # Probabilities should be [prob of 0, prob of 1], return prob of 1\n","\n","# Apply and store predicted probability\n","df_test = pd.DataFrame(ds['test'])\n","df_test['pred_prob'] = df_test['body'].map(predict)\n","\n","#df_test = pd.DataFrame(ds['test'])\n","#df_test['prediction'] = df_test['body'].map(predict)\n","#df_test['y_pred'] = df_test['prediction'].apply(lambda x: np.argmax(x, axis=0))"],"metadata":{"trusted":true,"id":"9f93f3ee-23b1-4f27-98f7-2fb9d6ecedad"},"outputs":[],"execution_count":null},{"id":"ff3633ab-b116-4f2d-90c3-56a9857d1982","cell_type":"code","source":["# Calculate and print the elapsed time\n","elapsed_time = time.time() - start_time\n","print(f\"Time required by inference: {elapsed_time:.4f} seconds\")"],"metadata":{"trusted":true,"id":"ff3633ab-b116-4f2d-90c3-56a9857d1982"},"outputs":[],"execution_count":null},{"id":"d30c2c24-ee7d-4838-81c7-30da9c60a7f9","cell_type":"markdown","source":["### Use ROC to determine best threshold for class 1"],"metadata":{"id":"d30c2c24-ee7d-4838-81c7-30da9c60a7f9"}},{"id":"b3fbce6f-8437-4316-840c-a9908e3a608e","cell_type":"code","source":["from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve\n","\n","# Get true labels and predicted probabilities\n","y_true = df_test['label'].values\n","y_probs = df_test['pred_prob'].values\n","\n","# Search for best threshold by maximizing F1-score\n","thresholds = np.arange(0.0, 1.01, 0.01)\n","f1_scores = [f1_score(y_true, y_probs > t) for t in thresholds]\n","best_threshold = thresholds[np.argmax(f1_scores)]\n","\n","print(f\"Best threshold based on F1-score: {best_threshold:.2f}\")\n","\n","#test_pred = df_test['y_pred'].to_numpy()\n","#y_true = df_test['label']\n","#y_scores = df_test['prediction'].apply(lambda x: x[1])  # probability of class 1\n"],"metadata":{"trusted":true,"id":"b3fbce6f-8437-4316-840c-a9908e3a608e"},"outputs":[],"execution_count":null},{"id":"70ef9d77-f5d3-4ddc-9ff3-238a0fbb2fbe","cell_type":"markdown","source":["### Classification metrics on Test data"],"metadata":{"id":"70ef9d77-f5d3-4ddc-9ff3-238a0fbb2fbe"}},{"id":"740410e6-b98d-41da-a1d3-38e483d21ca0","cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","# Convert probabilities to labels based on best threshold\n","df_test['y_pred'] = (df_test['pred_prob'] >= best_threshold).astype(int)\n","\n","# Print classification report\n","print(classification_report(y_true, df_test['y_pred']))\n","\n","\n","#from sklearn.metrics import classification_report\n","#print('classifiation report')\n","#print(classification_report(test_pred, df_test['label'].to_numpy(),target_names=label_names))"],"metadata":{"trusted":true,"id":"740410e6-b98d-41da-a1d3-38e483d21ca0"},"outputs":[],"execution_count":null},{"id":"9a4c7e7e-36bd-42e3-8903-05b52472bc59","cell_type":"markdown","source":["#### AUC"],"metadata":{"id":"9a4c7e7e-36bd-42e3-8903-05b52472bc59"}},{"id":"4119474e-985b-4a23-ae77-a126c909d2b3","cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","precision, recall, pr_thresholds = precision_recall_curve(y_true, y_probs)\n","\n","plt.plot(pr_thresholds, precision[:-1], label='Precision')\n","plt.plot(pr_thresholds, recall[:-1], label='Recall')\n","plt.axvline(best_threshold, color='r', linestyle='--', label=f'Best threshold: {best_threshold:.2f}')\n","plt.xlabel(\"Threshold\")\n","plt.ylabel(\"Score\")\n","plt.title(\"Precision-Recall vs Threshold\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","\n","# import matplotlib.pyplot as plt\n","# from sklearn.metrics import roc_curve, auc\n","\n","# positive_probs = df_test['prediction'].apply(lambda x: x[1])\n","\n","# # Compute ROC curve\n","# fpr, tpr, _ = roc_curve(test_pred, positive_probs)\n","# roc_auc = auc(fpr, tpr)\n","\n","# # Plot\n","# plt.figure(figsize=(8, 6))\n","# plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n","# plt.plot([0, 1], [0, 1], color='grey', linestyle='--')  # Diagonal line for random chance\n","# plt.xlim([0.0, 1.0])\n","# plt.ylim([0.0, 1.05])\n","# plt.xlabel('False Positive Rate')\n","# plt.ylabel('True Positive Rate')\n","# plt.title('Receiver Operating Characteristic (ROC) Curve')\n","# plt.legend(loc=\"lower right\")\n","# plt.show()"],"metadata":{"trusted":true,"id":"4119474e-985b-4a23-ae77-a126c909d2b3"},"outputs":[],"execution_count":null},{"id":"ccc2fedd-e18c-43f7-9887-49430123c2b3","cell_type":"markdown","source":["### Example Inferences"],"metadata":{"id":"ccc2fedd-e18c-43f7-9887-49430123c2b3"}},{"id":"dccf5df9-9b93-4295-a0fc-9d6ddaf56915","cell_type":"markdown","source":["test_df['pred'] = test_pred\n","test_df.reset_index(level=0)\n","print(test_df[test_df['label']==test_df['pred']].shape)\n","test_df[test_df['label']==test_df['pred']][['text','label','pred']].head(100)"],"metadata":{"id":"dccf5df9-9b93-4295-a0fc-9d6ddaf56915"}}]}